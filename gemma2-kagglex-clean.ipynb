{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d1ca25-bd9e-4d17-abf6-f948ff39e0c5",
   "metadata": {},
   "source": [
    "# Large Language Model Fundamentals & Practical Applications (Demo)\n",
    "## KaggleX Speaker Workshop\n",
    "### By: David Ramirez ([GitHub](https://github.com/dframirez-usmc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "727c3b07-e876-49f6-969d-421554a3daa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gemma-2 information:\n",
    "\n",
    "# Google Model Card\n",
    "# https://ai.google.dev/gemma/docs/model_card_2\n",
    "\n",
    "# HuggingFace\n",
    "# https://huggingface.co/google/gemma-2-2b\n",
    "\n",
    "# Kaggle\n",
    "# https://www.kaggle.com/models/google/gemma-2/transformers/gemma-2-2b\n",
    "\n",
    "# Best guess on memory requirements\n",
    "# https://huggingface.co/google/gemma-2b/discussions/59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574eb568-285b-4bf8-82e9-9ca196d6179e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You must agree to the use terms of the Gemma model\n",
    "# You must create your own Hugging Face Access Token\n",
    "access_token = \"YOUR_HUGGINGFACE_ACCESS_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc8549-3f08-451c-9bce-e03546e97044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You may download the model and code through git from Hugging Face\n",
    "# NOTE: Replace with your Hugging Face USERNAME and ACCESSTOKEN\n",
    "!git clone https://USERNAME:ACCESSTOKEN@huggingface.co/google/gemma-2-2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6359c45e-730b-4c49-a258-d708a14ed1f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep  5 15:29:31 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   36C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check for Nvidia GPU (System Management Interface)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26fd8993-6863-4adc-9c0e-24860e1912d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n"
     ]
    }
   ],
   "source": [
    "# Check for Nvidia CUDA (Nvidia CUDA Compiler)\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f667ef8-f0b3-4bec-b0cf-f0c2f24945ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Keep your pip package up to date\n",
    "!pip install pip --upgrade #--quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd16c5-c9e2-4ac4-99d3-306afb2c93e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers accelerate --upgrade #--quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c443ec2-ca14-4e68-97b6-51bfe4155600",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This pip install assumes you have CUDA 12.4 installed\n",
    "# CUDA setup is usually seperate from python or pip\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 --upgrade #--quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec7448bc-6efb-47d6-baf7-7fe28ad2a66e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It may be necessary to uninstall in order to reinstall PyTorch\n",
    "#!pip uninstall torch torchvision torchaudio --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562793bc-71df-4db2-ab45-274472ce7901",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check against your installed pip packages\n",
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494d6f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check against your installed pip packages\n",
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "684981aa-d2f2-42aa-ab5c-fea4af782f35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f13a8a2-fd81-440f-8758-8ec166b1d903",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e28f018c-fc02-49c6-8d5e-71668767ea39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla T4'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86b038b0-bc85-426c-b427-374363941e4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c41d871e734d88861021b22fe323a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model and tokenizer directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Gemma2ForCausalLM\n",
    "\n",
    "model_id = \"google/gemma-2-2b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token)\n",
    "model = Gemma2ForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    token=access_token,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52a3b1fb-b0f5-49fc-a2f7-8e847610df32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf198833-7940-4335-85a1-041d1eeb64d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text):  \n",
    "\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    print(\"input_ids =\")\n",
    "    print(input_ids.cpu().numpy()[0])\n",
    "    print(\"Input token IDs\", end=\"\\n\\n\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids, \n",
    "        max_new_tokens=16,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True,\n",
    "        output_logits=True,\n",
    "    )\n",
    "    \n",
    "    logits = outputs.logits[0][0]\n",
    "    \n",
    "    print(\"len(output) =\")\n",
    "    print(len(logits.cpu().numpy()))\n",
    "    print(\"Quantity of words in the dictonary\", end=\"\\n\\n\")\n",
    "    \n",
    "    print(\"output.logits =\")\n",
    "    print(logits.cpu().numpy())\n",
    "    print(\"Output raw values\", end=\"\\n\\n\")\n",
    "    \n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    percents = [f'{i*100:.1f}%' for i in probs.cpu().numpy()]\n",
    "    print(\"softmax(output.logits) =\")\n",
    "    print(percents[0:3], end=\"\")\n",
    "    print(\" ... \", end=\"\")\n",
    "    print(percents[-4:-1])\n",
    "    print(\"Very small percentages\", end=\"\\n\\n\")\n",
    "    \n",
    "    print(\"argmax(softmax) =\")\n",
    "    print(torch.argmax(probs).cpu().numpy())\n",
    "    # TODO: Convert into percentages %\n",
    "    print(\"max(softmax) =\")\n",
    "    max_percent = '{:.2%}'.format(torch.max(probs).cpu().numpy())\n",
    "    print(max_percent)\n",
    "    print(\"Best next token ID and percent chance\", end=\"\\n\\n\")\n",
    "    \n",
    "    print(\"output =\")\n",
    "    print(outputs[0][0].cpu().numpy())\n",
    "    print(\"All output token IDs\", end=\"\\n\\n\")\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs.sequences[0])\n",
    "\n",
    "    # Format the code output\n",
    "    formatted_code = generated_text.strip() + \"\\n\"\n",
    "    print(formatted_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "245b68dd-cd0d-4fae-b6d0-d63b04c67f48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids =\n",
      "[    2  6353  8447 25166   877  5374   736  6218   577]\n",
      "Input token IDs\n",
      "\n",
      "len(output) =\n",
      "256000\n",
      "Quantity of words in the dictonary\n",
      "\n",
      "output.logits =\n",
      "[-15.9375    9.0625   -9.875   ...  -1.71875  -2.34375 -15.9375 ]\n",
      "Output raw values\n",
      "\n",
      "softmax(output.logits) =\n",
      "['0.0%', '0.0%', '0.0%'] ... ['0.0%', '0.0%', '0.0%']\n",
      "Very small percentages\n",
      "\n",
      "argmax(softmax) =\n",
      "573\n",
      "max(softmax) =\n",
      "14.08%\n",
      "Best next token ID and percent chance\n",
      "\n",
      "output =\n",
      "[     2   6353   8447  25166    877   5374    736   6218    577    573\n",
      "   1580    576    573   1162 235265    109    651   3712    576    573\n",
      "   2351   8447    603   5043    577]\n",
      "All output token IDs\n",
      "\n",
      "<bos>Next token prediction will continue this statement to the end of the year.\n",
      "\n",
      "The price of the next token is expected to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_text(\"Next token prediction will continue this statement to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fbda61-6b05-46d2-9efb-45c0de7ea499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3f32a2-83ca-4ed0-8c8f-d2fd456fa874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean up the GPU usage to run a different way\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c4f8f4-5124-4253-8288-0c0467233eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use a pipeline instead of model+tokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"google/gemma-2-2b\"\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    do_sample=False,\n",
    "    token=access_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e16a7-9937-41db-90d7-60f4d4040bad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = pipeline(\n",
    "    \"Next token prediction will continue this statement\",\n",
    "    max_new_tokens=16,\n",
    "    do_sample=False\n",
    ")\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
